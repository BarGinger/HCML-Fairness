{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvGd4YgtppjR"
   },
   "source": [
    "## Submission instructions\n",
    "\n",
    "All code that you write should be in this notebook. Please include your names and student numbers. You have to submit this notebook, with your code and answers filled in. Make sure to add enough documentation.\n",
    "\n",
    "For questions, make use of the \"Lab\" session (see schedule).\n",
    "Questions can also be posted to the MS teams channel called \"Lab\".\n",
    "\n",
    "**Note:** You are free to make use of Python libraries (e.g., numpy, sklearn, etc.) except any *fairness* libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P9dsbAffgfZ"
   },
   "source": [
    "#### Name and student numbers\n",
    "Bar Melinarskiy - 2482975\n",
    "\n",
    "Julia Baas - 6082826\n",
    "\n",
    "**Group number**: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ni3V-7iqA6X"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In this assignment we are going to use the **COMPAS** dataset.\n",
    "\n",
    "If you haven't done so already, take a look at this article: https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n",
    "For background on the dataset, see https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.\n",
    "\n",
    "**Reading in the COMPAS dataset**\n",
    "\n",
    "The dataset can be downloaded here: https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv\n",
    "\n",
    "For this assignment, we focus on the protected attribute *race*.\n",
    "\n",
    "The label (the variable we want to be able to predict) represents recidivism, which is defined as a new arrest within 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajLXEdx6plgP",
    "outputId": "a65bf36a-1bc6-488f-accc-7bd9b24ca411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget -c https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AaT9DQwwpqkx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "compas_data = pd.read_csv('compas-scores-two-years.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsidUr4Bz-gZ"
   },
   "source": [
    "We apply several data preprocessing steps, including only retaining Caucasians and African Americans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vs2eRxdgrHrt"
   },
   "outputs": [],
   "source": [
    "compas_data = compas_data[(compas_data.days_b_screening_arrest <= 30)\n",
    "            & (compas_data.days_b_screening_arrest >= -30)\n",
    "            & (compas_data.is_recid != -1)\n",
    "            & (compas_data.c_charge_degree != 'O')\n",
    "            & (compas_data.score_text != 'N/A')\n",
    "            & ((compas_data.race == 'Caucasian') | (compas_data.race == 'African-American'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5IwB6Rz2zIS"
   },
   "source": [
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMM-MYdstObf",
    "outputId": "0bf826a7-bf28-433b-ea6a-b21c5f3f1fe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id              name      first    last compas_screening_date     sex  \\\n",
      "1    3       kevon dixon      kevon   dixon            2013-01-27    Male   \n",
      "2    4          ed philo         ed   philo            2013-04-14    Male   \n",
      "6    8     edward riddle     edward  riddle            2014-02-19    Male   \n",
      "8   10  elizabeth thieme  elizabeth  thieme            2014-03-16  Female   \n",
      "10  14    benjamin franc   benjamin   franc            2013-11-26    Male   \n",
      "\n",
      "           dob  age       age_cat              race  ...  v_decile_score  \\\n",
      "1   1982-01-22   34       25 - 45  African-American  ...               1   \n",
      "2   1991-05-14   24  Less than 25  African-American  ...               3   \n",
      "6   1974-07-23   41       25 - 45         Caucasian  ...               2   \n",
      "8   1976-06-03   39       25 - 45         Caucasian  ...               1   \n",
      "10  1988-06-01   27       25 - 45         Caucasian  ...               4   \n",
      "\n",
      "    v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
      "1            Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
      "2            Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
      "6            Low        2014-02-19  2014-03-31   2014-04-18              14   \n",
      "8            Low        2014-03-16  2014-03-15   2014-03-18               0   \n",
      "10           Low        2013-11-26  2013-11-25   2013-11-26               0   \n",
      "\n",
      "   start  end event two_year_recid  \n",
      "1      9  159     1              1  \n",
      "2      0   63     0              1  \n",
      "6      5   40     1              1  \n",
      "8      2  747     0              0  \n",
      "10     0  857     0              0  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "print(compas_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vKapT6FtmvJ"
   },
   "source": [
    "Now take a look at the distribution of the protected attribute `race` and the distribution of our outcome variable `two_year_recid`.\n",
    "\n",
    "**Note:** in the context of fair machine learning, the favorable label here is no recidivism, i.e., ```two_year_recid = 0```. So think about how what you will code as the positive class in your machine learning experiments, and make sure your interpretation of the results is consistent with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh2oM2yptnjR",
    "outputId": "f89ff857-41d9-4a8c-d236-6fd4af65f177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances per race category:\n",
      "race              two_year_recid\n",
      "African-American  1                 1661\n",
      "                  0                 1514\n",
      "Caucasian         0                 1281\n",
      "                  1                  822\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of instances per race category:')\n",
    "print(compas_data[['race', 'two_year_recid']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "636Yopp6wNtY"
   },
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS3g2NT98dY_"
   },
   "source": [
    "### **1. Exploration**\n",
    "\n",
    "First we perform an exploratory analysis of the data.\n",
    "\n",
    "**Question:** What is the size of the data? (i.e. how many data instances does it contain?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "k6FESAE1VmPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5278, 53)\n",
      "Number of instances: 5278\n",
      "Number of features: 53\n",
      "Number of classes: 2\n",
      "Overall class distribution:\n",
      " two_year_recid  Count  Percentage\n",
      "              0   2795   52.955665\n",
      "              1   2483   47.044335\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "print(f\"Dataset shape: {compas_data.shape}\")\n",
    "dataset_size = len(compas_data)\n",
    "print(f\"Number of instances: {dataset_size}\")\n",
    "print(f\"Number of features: {len(compas_data.columns)}\")\n",
    "print(f\"Number of classes: {len(compas_data['two_year_recid'].unique())}\")\n",
    "\n",
    "print(\"Overall class distribution:\")\n",
    "distribution = compas_data['two_year_recid'].value_counts().reset_index()\n",
    "distribution.columns = ['two_year_recid', 'Count']\n",
    "distribution['Percentage'] = (distribution['Count'] / compas_data['two_year_recid'].count()) * 100\n",
    "print(distribution.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:*** \n",
    "We have 5,278 instances in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx-EDiJ41-_O"
   },
   "source": [
    "**Question:** In the dataset, the protected attribute is `race`, which has two categories: White and African Americans. How many data instances belong to each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e9WqGTz5237Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall race distribution:\n",
      "             race  Count  Percentage\n",
      " African-American   3175   60.155362\n",
      "        Caucasian   2103   39.844638\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "print(\"Overall race distribution:\")\n",
    "feature = \"race\"\n",
    "distribution = compas_data[feature].value_counts().reset_index()\n",
    "distribution.columns = [feature, 'Count']\n",
    "distribution['Percentage'] = (distribution['Count'] / compas_data[feature].count()) * 100\n",
    "print(distribution.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLdMWSNK3OPt"
   },
   "source": [
    "**Question:** What are the base rates (the probability of a favorable outcome for the two protected attribute classes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3OoqKyud3jIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of the label (two_year_recid) by the protected feature (race):\n",
      "             race  Count_two_year_recid_0  Count_two_year_recid_1  Probability_two_year_recid_0  Probability_two_year_recid_1\n",
      " African-American                    1514                    1661                       0.47685                       0.52315\n",
      "        Caucasian                    1281                     822                       0.60913                       0.39087\n",
      "\n",
      "SO the base rates (two_year_recid = 0) and counts are:\n",
      "For African-American: Count=1514, Probability=0.477\n",
      "For Caucasian: Count=1281, Probability=0.609\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "protected_feature = \"race\"\n",
    "label = \"two_year_recid\"\n",
    "\n",
    "# Grouping by the protected feature and calculate the counts and proportions for each label value\n",
    "counts = compas_data.groupby(protected_feature)[label].value_counts(normalize=False).unstack().fillna(0)\n",
    "proportions = compas_data.groupby(protected_feature)[label].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Resetting the index for better readability\n",
    "counts = counts.reset_index()\n",
    "proportions = proportions.reset_index()\n",
    "\n",
    "# Renaming the columns for clarity\n",
    "counts.columns = [protected_feature] + [f\"Count_{label}_{i}\" for i in counts.columns[1:]]\n",
    "proportions.columns = [protected_feature] + [f\"Probability_{label}_{i}\" for i in proportions.columns[1:]]\n",
    "\n",
    "# Combining counts and proportions into a single DataFrame\n",
    "distribution = pd.merge(counts, proportions, on=protected_feature)\n",
    "\n",
    "# Printing the combined DataFrame without the index\n",
    "print(\"\\nDistribution of the label (two_year_recid) by the protected feature (race):\")\n",
    "print(distribution.to_string(index=False))\n",
    "\n",
    "# Printing the base rates and counts for the favorable outcome (two_year_recid = 0)\n",
    "print(\"\\nSO the base rates (two_year_recid = 0) and counts are:\")\n",
    "for _, row in distribution.iterrows():\n",
    "    print(f\"For {row[protected_feature]}: Count={row[f'Count_{label}_0']}, Probability={row[f'Probability_{label}_0']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr-0I7It3pBY"
   },
   "source": [
    "**Question:** What are the base rates for the combination of both race and sex categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3SCmOWs43t9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of the label (two_year_recid) by the protected features (race, sex):\n",
      "             race     sex  Count_two_year_recid_0  Count_two_year_recid_1  Probability_two_year_recid_0  Probability_two_year_recid_1\n",
      " African-American  Female                     346                     203                      0.630237                      0.369763\n",
      " African-American    Male                    1168                    1458                      0.444783                      0.555217\n",
      "        Caucasian  Female                     312                     170                      0.647303                      0.352697\n",
      "        Caucasian    Male                     969                     652                      0.597779                      0.402221\n",
      "\n",
      "SO the base rates (two_year_recid = 0) and counts are:\n",
      "For race=African-American, sex=Female: Count=346, Probability=0.630\n",
      "For race=African-American, sex=Male: Count=1168, Probability=0.445\n",
      "For race=Caucasian, sex=Female: Count=312, Probability=0.647\n",
      "For race=Caucasian, sex=Male: Count=969, Probability=0.598\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "protected_features = [\"race\", \"sex\"]\n",
    "label = \"two_year_recid\"\n",
    "      \n",
    "# Grouping by the protected features and calculate the counts and proportions for each label value\n",
    "distribution = compas_data.groupby(protected_features)[label].value_counts(normalize=False).unstack().fillna(0)\n",
    "proportions = compas_data.groupby(protected_features)[label].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Combining counts and proportions into a single DataFrame\n",
    "distribution = distribution.reset_index()\n",
    "proportions = proportions.reset_index()\n",
    "distribution.columns = protected_features + [f\"Count_{label}_{i}\" for i in distribution.columns[len(protected_features):]]\n",
    "proportions.columns = protected_features + [f\"Probability_{label}_{i}\" for i in proportions.columns[len(protected_features):]]\n",
    "combined = pd.merge(distribution, proportions, on=protected_features)\n",
    "\n",
    "# Printing the combined DataFrame without the index\n",
    "print(\"\\nDistribution of the label (two_year_recid) by the protected features (race, sex):\")\n",
    "\n",
    "print(combined.to_string(index=False))\n",
    "\n",
    "# Printing the base rates and counts for the favorable outcome (two_year_recid = 0)\n",
    "print(\"\\nSO the base rates (two_year_recid = 0) and counts are:\")\n",
    "for _, row in combined.iterrows():\n",
    "    feature_combination = \", \".join([f\"{feature}={row[feature]}\" for feature in protected_features])\n",
    "    print(f\"For {feature_combination}: Count={row[f'Count_{label}_0']}, Probability={row[f'Probability_{label}_0']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2qyyoS74UGC"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Write down a short interpretation of the statistics you calculated. What do you see?\n",
    "> **Answer: Female offenders are less likely to become a recidivist than male offenders (0.630 over 0.445 and 0.647 over 0.598 for African-American and Caucasion respectively). Furthermore, African-American offenders are more likely to become a recidivist than Caucasian offenders (0.445 over 0.598 and 0.630 over 0.647 for male and female respectively).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-mxqnnUcaGc"
   },
   "source": [
    "### **2. Performance measures**\n",
    "\n",
    "You will have to measure the performance and fairness of different classifiers in question 5. The performance will be calculated with the precision, recall, F1 and accuracy.\n",
    "Additionally, you will have to calculate the statistical/demographic parity, the true positive rate (recall) and false positive rate per race group.\n",
    "\n",
    "Make sure that you are able to calculate these metrics in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IAO99gf2caZT"
   },
   "outputs": [],
   "source": [
    "# Your code for the performance measures\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "favorable_outcome = 0  # Define the favorable outcome (e.g., 0 for 'not recidivist')\n",
    "unfavorable_outcome = 1  # Define the unfavorable outcome (e.g., 1 for 'recidivist')\n",
    "\n",
    "# Function to calculate overall performance metrics\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall (TPR)\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "    df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "    return df\n",
    "\n",
    "# Function to calculate statistical/demographic parity\n",
    "def calculate_statistical_parity(x, y_pred, protected_attributes):\n",
    "    parity = {}\n",
    "    y_groups = {g: y_pred.loc[df.index] for g, df in x.groupby(protected_attributes)}\n",
    "\n",
    "\n",
    "    for key, group in y_groups.items():\n",
    "        favorable_rate = np.mean( group == favorable_outcome)  # Favorable outcome is `0`\n",
    "        parity[key] = favorable_rate\n",
    "    df = pd.DataFrame.from_dict(parity, orient='index')\n",
    "    df.columns = ['Favorable Outcome Rate']\n",
    "\n",
    "    if len(protected_attributes) == 1:\n",
    "        df.index = df.index.map(lambda x: 'Caucasian' if x == 1 else 'African-American')\n",
    "    else:\n",
    "        df.index = df.index.map({\n",
    "            (0,0): 'African-American Female',\n",
    "            (0, 1): 'African-American Men',\n",
    "            (1,0): 'Caucasian Female',\n",
    "            (1,1): 'Caucasian Men'\n",
    "        })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to calculate TPR and FPR per group\n",
    "def calculate_group_metrics(X, y_true, y_pred, protected_attributes):\n",
    "    group_metrics = {}\n",
    "    # Group the data by the protected attribute    \n",
    "    X_groups = {g: df.drop(columns=protected_attributes) for g, df in X.groupby(protected_attributes)}\n",
    "    \n",
    "    # Use the indices from the grouped X to split y_true and y_pred\n",
    "    y_true_groups = {g: y_true.loc[df.index] for g, df in X_groups.items()}\n",
    "    y_pred_groups = {g: y_pred.loc[df.index] for g, df in X_groups.items()}\n",
    "\n",
    "\n",
    "    for group in X_groups.keys():\n",
    "        # Get the corresponding y_true and y_pred groups\n",
    "        y_true_group = y_true_groups[group]\n",
    "        y_pred_group = y_pred_groups[group]\n",
    "        # True Positive Rate (TPR)\n",
    "        tpr = recall_score(y_true_group, y_pred_group, zero_division=0)\n",
    "        \n",
    "        # False Positive Rate (FPR)\n",
    "        fp = np.sum((y_pred_group == unfavorable_outcome) & (y_true_group == favorable_outcome))\n",
    "        tn = np.sum((y_pred_group == favorable_outcome) & (y_true_group == favorable_outcome))\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        group_metrics[group] = {\"TPR\": tpr, \"FPR\": fpr}\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    group_metrics_df = pd.DataFrame.from_dict(group_metrics, orient='index')\n",
    "    \n",
    "    # Set the index to meaningful labels\n",
    "    if len(protected_attributes) == 1:\n",
    "        group_metrics_df.index = \\\n",
    "            group_metrics_df.index.map(lambda x: 'Caucasian' if x == 1 else 'African-American')\n",
    "\n",
    "    else:\n",
    "        group_metrics_df.index = group_metrics_df.index.map({\n",
    "            (0,0): 'African-American Female',\n",
    "            (0, 1): 'African-American Men',\n",
    "            (1,0): 'Caucasian Female',\n",
    "            (1,1): 'Caucasian Men'\n",
    "        })\n",
    "\n",
    "\n",
    "    return group_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n51Bdpy59vhy"
   },
   "source": [
    "### **3. Prepare the data**\n",
    "For the classifiers in question 5, the input of the model can only contain numerical values, it is therefore important to convert the strings in the columns (features) of interest of the `compas_data` to floats or integers.\n",
    "\n",
    "The columns of interest are features that you think will be informative or interesting in predicting the outcome variable.\n",
    "Use the cell below to explore which of the Compas variables you need to convert to be able to use them for the classifiers.\n",
    "\n",
    "Generate a new dataframe with your selected features in the right encoding (also make sure to include `two_year_recid`). You can implement this yourself, or use the `LabelEncoder` from `sklearn`.\n",
    "\n",
    "**Note:** you do not need to convert all columns/features, only the ones you are interested in. However, do **not** include the feature `is_recid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5278 entries, 1 to 7212\n",
      "Data columns (total 53 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       5278 non-null   int64  \n",
      " 1   name                     5278 non-null   object \n",
      " 2   first                    5278 non-null   object \n",
      " 3   last                     5278 non-null   object \n",
      " 4   compas_screening_date    5278 non-null   object \n",
      " 5   sex                      5278 non-null   object \n",
      " 6   dob                      5278 non-null   object \n",
      " 7   age                      5278 non-null   int64  \n",
      " 8   age_cat                  5278 non-null   object \n",
      " 9   race                     5278 non-null   object \n",
      " 10  juv_fel_count            5278 non-null   int64  \n",
      " 11  decile_score             5278 non-null   int64  \n",
      " 12  juv_misd_count           5278 non-null   int64  \n",
      " 13  juv_other_count          5278 non-null   int64  \n",
      " 14  priors_count             5278 non-null   int64  \n",
      " 15  days_b_screening_arrest  5278 non-null   float64\n",
      " 16  c_jail_in                5278 non-null   object \n",
      " 17  c_jail_out               5278 non-null   object \n",
      " 18  c_case_number            5278 non-null   object \n",
      " 19  c_offense_date           4589 non-null   object \n",
      " 20  c_arrest_date            689 non-null    object \n",
      " 21  c_days_from_compas       5278 non-null   float64\n",
      " 22  c_charge_degree          5278 non-null   object \n",
      " 23  c_charge_desc            5273 non-null   object \n",
      " 24  is_recid                 5278 non-null   int64  \n",
      " 25  r_case_number            2647 non-null   object \n",
      " 26  r_charge_degree          2647 non-null   object \n",
      " 27  r_days_from_arrest       1781 non-null   float64\n",
      " 28  r_offense_date           2647 non-null   object \n",
      " 29  r_charge_desc            2606 non-null   object \n",
      " 30  r_jail_in                1781 non-null   object \n",
      " 31  r_jail_out               1781 non-null   object \n",
      " 32  violent_recid            0 non-null      float64\n",
      " 33  is_violent_recid         5278 non-null   int64  \n",
      " 34  vr_case_number           612 non-null    object \n",
      " 35  vr_charge_degree         612 non-null    object \n",
      " 36  vr_offense_date          612 non-null    object \n",
      " 37  vr_charge_desc           612 non-null    object \n",
      " 38  type_of_assessment       5278 non-null   object \n",
      " 39  decile_score.1           5278 non-null   int64  \n",
      " 40  score_text               5278 non-null   object \n",
      " 41  screening_date           5278 non-null   object \n",
      " 42  v_type_of_assessment     5278 non-null   object \n",
      " 43  v_decile_score           5278 non-null   int64  \n",
      " 44  v_score_text             5278 non-null   object \n",
      " 45  v_screening_date         5278 non-null   object \n",
      " 46  in_custody               5278 non-null   object \n",
      " 47  out_custody              5278 non-null   object \n",
      " 48  priors_count.1           5278 non-null   int64  \n",
      " 49  start                    5278 non-null   int64  \n",
      " 50  end                      5278 non-null   int64  \n",
      " 51  event                    5278 non-null   int64  \n",
      " 52  two_year_recid           5278 non-null   int64  \n",
      "dtypes: float64(4), int64(16), object(33)\n",
      "memory usage: 2.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(compas_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2G0-QxbH95rX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5278 entries, 1 to 7212\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype\n",
      "---  ------          --------------  -----\n",
      " 0   sex             5278 non-null   int32\n",
      " 1   age             5278 non-null   int64\n",
      " 2   race            5278 non-null   int32\n",
      " 3   priors_count    5278 non-null   int64\n",
      " 4   juv_fel_count   5278 non-null   int64\n",
      " 5   two_year_recid  5278 non-null   int64\n",
      "dtypes: int32(2), int64(4)\n",
      "memory usage: 247.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#filtering only the features we find interesting:\n",
    "columns_to_keep = [\"sex\", \"age\", \"race\", \"priors_count\", \"juv_fel_count\", \"two_year_recid\"]\n",
    "new_dataset = compas_data[columns_to_keep].copy()\n",
    "\n",
    "#Converting sex values into 0 and 1\n",
    "new_dataset[\"sex\"] = (new_dataset[\"sex\"] == 'Male').astype(int) #Male = 1, female = 0\n",
    "new_dataset[\"race\"] = (new_dataset[\"race\"] == 'Caucasian').astype(int) #Caucasian = 1, A-A = 0\n",
    "\n",
    "#check it:\n",
    "print(new_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-ILbSsR-ZW_"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Give a short motivation (one-two sentence) per feature why you think this is informative or interesting to take into account.\n",
    "> Answer: Sex is important to include because we saw that the reoffender scores differed, so it could be an indicator. Furthermore, we wanted to include race since this was an important topic in the discussion on the COMPAS system. We also wanted to include age because young offenders have more time to reoffend (because they'll probably live longer) and are more likely to reoffend. This was also pointed out by ProPublica. The amount of prior offenses can also be a good indicator, since the offender already is a reoffender. The same logic holds for juvenile felony counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkgRPopIxNVJ"
   },
   "source": [
    "### **4. Train and test split**\n",
    "\n",
    "Divide the dataset into a train (80%) and test split (20%), either by implementing it yourself, or by using an existing library.\n",
    "\n",
    "**Note:** Usually when carrying out machine learning experiments,\n",
    "we also need a dev set for developing and selecting our models (incl. tuning of hyper-parameters).\n",
    "However, in this assignment, the goal is not to optimize\n",
    "the performance of models so we'll only use a train and test split.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G0wUGEpiV7mH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4222 instances\n",
      "Testing set size: 1056 instances\n"
     ]
    }
   ],
   "source": [
    "# Your code to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features (X) and the target variable (y)\n",
    "label = \"two_year_recid\"\n",
    "X = new_dataset.drop(columns=[label])  # Drop the target column\n",
    "y = new_dataset[label]  # Target column\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) setsm \n",
    "# using stratified sampling to maintain the distribution of the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]} instances\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmq45GgAWEJo"
   },
   "source": [
    "### **5. Classifiers**\n",
    "\n",
    "Now, train and test different classifiers and report the following statistics:\n",
    "\n",
    "* Overall performance:\n",
    "\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * F1\n",
    "  * Accuracy\n",
    "\n",
    "* Fairness performance:\n",
    "\n",
    "  * The statistical parity difference for the protected attribute `race`(i.e. the difference in the probability of receiving a favorable label between the two protected attribute groups);\n",
    "  * The true positive rates of the two protected attribute groups\n",
    "  * The false positive rates of the two protected attribute groups.\n",
    "\n",
    "For training the classifier we recommend using scikit-learn (https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu6eQ_3xGfXd"
   },
   "source": [
    "#### **5.1 Regular classification**\n",
    "Train a logistic regression classifier with the race feature and all other features that you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xDaGo07EWElK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logstic Regression with all chosen features\n",
      "\n",
      "****************************************************************\n",
      "The overall scores are:\n",
      "                 Value\n",
      "Precision     0.661327\n",
      "Recall (TPR)  0.581489\n",
      "F1 Score      0.618844\n",
      "Accuracy      0.662879\n",
      "\n",
      "****************************************************************\n",
      "The statistical parity difference for race is:\n",
      "                  Favorable Outcome Rate\n",
      "African-American                0.472089\n",
      "Caucasian                       0.752914\n",
      "\n",
      "****************************************************************\n",
      "The TPR and FPR of the two protected attribute groups:\n",
      "                       TPR       FPR\n",
      "African-American  0.678899  0.363333\n",
      "Caucasian         0.394118  0.150579\n"
     ]
    }
   ],
   "source": [
    "# Your code for classifier 1\n",
    "import pprint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "protected_features = ['race']\n",
    "\n",
    "# train the logistic regression model\n",
    "clf1 = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred_1 = pd.Series(clf1.predict(X_test))\n",
    "\n",
    "# Reset the indices of y_test and y_pred to match the original test dateset DataFrame\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "y_pred_1 = y_pred_1.reset_index(drop=True)\n",
    "\n",
    "print(\"Logstic Regression with all chosen features\")\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The overall scores are:\")\n",
    "df_1_1 = calculate_performance_metrics(y_test, y_pred_1)\n",
    "print(df_1_1.to_string())\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The statistical parity difference for race is:\")\n",
    "df_1_2 = calculate_statistical_parity(X_test, y_pred_1, protected_features)\n",
    "pprint.pprint(df_1_2)\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The TPR and FPR of the two protected attribute groups:\")\n",
    "df_1_3 = calculate_group_metrics(X_test, y_test, y_pred_1, protected_features)\n",
    "pprint.pprint(df_1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM23AP8nFisw"
   },
   "source": [
    "#### **5.2 Without the protected attribute**\n",
    "Train a logistic regression classifier without the race feature, but with all other features you used in 5.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WL7LSSMPFmhv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logstic Regression without protected attribute (race)\n",
      "\n",
      "****************************************************************\n",
      "The overall scores are:\n",
      "                 Value\n",
      "Precision     0.657658\n",
      "Recall (TPR)  0.587525\n",
      "F1 Score      0.620616\n",
      "Accuracy      0.661932\n",
      "\n",
      "****************************************************************\n",
      "The statistical parity difference for race is:\n",
      "                  Favorable Outcome Rate\n",
      "African-American                0.473684\n",
      "Caucasian                       0.734266\n",
      "\n",
      "****************************************************************\n",
      "The TPR and FPR of the two protected attribute groups:\n",
      "                       TPR       FPR\n",
      "African-American  0.675841  0.363333\n",
      "Caucasian         0.417647  0.166023\n"
     ]
    }
   ],
   "source": [
    "# Your code for classifier 2\n",
    "import pprint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#filtering the data without race:\n",
    "protected_features = ['race']\n",
    "X_without_protected = X_train.drop(columns=protected_features)\n",
    "X_test_without_protected = X_test.drop(columns=protected_features)\n",
    "\n",
    "# train the logistic regression model\n",
    "clf2 = LogisticRegression(random_state=0).fit(X_without_protected, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred_without = pd.Series(clf2.predict(X_test_without_protected))\n",
    "\n",
    "# Reset the indices of y_test and y_pred to match the original test dateset DataFrame\n",
    "X_test_without_protected = X_test_without_protected.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "y_pred_without = y_pred_without.reset_index(drop=True)\n",
    "\n",
    "print(\"Logstic Regression without protected attribute (race)\")\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The overall scores are:\")\n",
    "df_2_1 = calculate_performance_metrics(y_test, y_pred_without)\n",
    "print(df_2_1.to_string())\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The statistical parity difference for race is:\")\n",
    "df_2_2 = calculate_statistical_parity(X_test, y_pred_without, protected_features)\n",
    "pprint.pprint(df_2_2)\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The TPR and FPR of the two protected attribute groups:\")\n",
    "df_2_3 = calculate_group_metrics(X_test, y_test, y_pred_without, protected_features)\n",
    "pprint.pprint(df_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5H_mlbM6qij9"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Write down a short interpretation of the results you calculated. What do you see?\n",
    "> Answer: The result is very similar. This means that adding the race doesn't make the system stronger. This is because race is not a good indicator for recidivism, the other features are stronger indicators. The outcome relies mostly on the other features, such as the amount of prior offenses. \n",
    "The result could be also explained by the fact that ProPublica's analysis was inaccurate. This was pointed out by many other scholars as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwiX0vEXFlgU"
   },
   "source": [
    "#### **5.3 Pre-processing: Reweighing**\n",
    "Train and test a classifier with weights (see lecture slide for the weight calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "G-1Vw1_xk4aQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_c = 0.8674773180460935\n",
      "weight_aa = 1.1115635757470286\n",
      "weight_nf_c = 1.2077272714062768\n",
      "weight_nf_aa = 0.8984721282182213\n",
      "Logstic Regression with all chosen features with sample weights\n",
      "\n",
      "****************************************************************\n",
      "The overall scores are:\n",
      "                 Value\n",
      "Precision     0.642157\n",
      "Recall (TPR)  0.527163\n",
      "F1 Score      0.579006\n",
      "Accuracy      0.639205\n",
      "\n",
      "****************************************************************\n",
      "The statistical parity difference for race is:\n",
      "                  Favorable Outcome Rate\n",
      "African-American                0.650718\n",
      "Caucasian                       0.559441\n",
      "\n",
      "****************************************************************\n",
      "The TPR and FPR of the two protected attribute groups:\n",
      "                       TPR       FPR\n",
      "African-American  0.513761  0.170000\n",
      "Caucasian         0.552941  0.366795\n"
     ]
    }
   ],
   "source": [
    "protected_features = ['race']\n",
    "\n",
    "#calculating the weights\n",
    "total_amount_of_offenders = len(X_train)\n",
    "amount_of_caucasians = len(X_train[X_train['race']==1])\n",
    "amount_of_african_americans = len(X_train[X_train['race']==0])\n",
    "amount_of_non_recidivists = len(y_train[y_train==0])\n",
    "amount_of_recidivists = len(y_train[y_train==1])\n",
    "amount_of_c_nr = len(X_train[(X_train['race']==1) & (y_train==0)])\n",
    "amount_of_aa_nr = len(X_train[(X_train['race']==0) & (y_train==0)])\n",
    "amount_of_c_r = len(X_train[(X_train['race']==1) & (y_train==1)])\n",
    "amount_of_aa_r = len(X_train[(X_train['race']==0) & (y_train==1)])\n",
    "\n",
    "#favorable weights calculation\n",
    "expected_c = (amount_of_caucasians/total_amount_of_offenders)*(amount_of_non_recidivists/total_amount_of_offenders)\n",
    "expected_aa = (amount_of_african_americans/total_amount_of_offenders)*(amount_of_non_recidivists/total_amount_of_offenders)\n",
    "observed_c = amount_of_c_nr/total_amount_of_offenders\n",
    "observed_aa = amount_of_aa_nr/total_amount_of_offenders\n",
    "weight_c = expected_c/observed_c\n",
    "weight_aa = expected_aa/observed_aa\n",
    "\n",
    "#non favorable weights calculation\n",
    "expected_nf_c = (amount_of_caucasians/total_amount_of_offenders)*(amount_of_recidivists/total_amount_of_offenders)\n",
    "expected_nf_aa = (amount_of_african_americans/total_amount_of_offenders)*(amount_of_recidivists/total_amount_of_offenders)\n",
    "observed_nf_c = amount_of_c_r/total_amount_of_offenders\n",
    "observed_nf_aa = amount_of_aa_r/total_amount_of_offenders\n",
    "weight_nf_c = expected_nf_c/observed_nf_c\n",
    "weight_nf_aa = expected_nf_aa/observed_nf_aa\n",
    "\n",
    "print(f\"weight_c = {weight_c}\")\n",
    "print(f\"weight_aa = {weight_aa}\")\n",
    "print(f\"weight_nf_c = {weight_nf_c}\")\n",
    "print(f\"weight_nf_aa = {weight_nf_aa}\")\n",
    "\n",
    "# Define weights for each combination of race and y_train\n",
    "weights = {\n",
    "    (0, 0): weight_aa,  # African-American, y=0\n",
    "    (0, 1): weight_nf_aa,  # African-American, y=1\n",
    "    (1, 0): weight_c,   # Caucasian, y=0\n",
    "    (1, 1): weight_nf_c    # Caucasian, y=1\n",
    "}\n",
    "# # Map weights based on both race and y_train\n",
    "sample_weights = X_train['race'].combine(y_train, lambda race, y: weights[(race, y)])\n",
    "# train the logistic regression model\n",
    "clf3 = LogisticRegression(random_state=0).fit(X_train, y_train, sample_weight = sample_weights)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred_3 = pd.Series(clf3.predict(X_test))\n",
    "\n",
    "# Reset the indices of y_test and y_pred to match the original test dateset DataFrame\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "y_pred_3 = y_pred_3.reset_index(drop=True)\n",
    "\n",
    "print(\"Logstic Regression with all chosen features with sample weights\")\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The overall scores are:\")\n",
    "df_3_1 = calculate_performance_metrics(y_test, y_pred_3)\n",
    "print(df_3_1.to_string())\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The statistical parity difference for race is:\")\n",
    "df_3_2 = calculate_statistical_parity(X_test, y_pred_3, protected_features)\n",
    "pprint.pprint(df_3_2)\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The TPR and FPR of the two protected attribute groups:\")\n",
    "df_3_3 = calculate_group_metrics(X_test, y_test, y_pred_3, protected_features)\n",
    "# df_2.index = df_2.index.map(lambda x: 'Caucasian' if x == 1 else 'African-American')\n",
    "pprint.pprint(df_3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yHv79yvv00a"
   },
   "source": [
    "**Question**\n",
    "\n",
    " Report the 4 weights that are used for reweighing and a short **interpretation/discussion** of the weights and the classifier results.\n",
    "> Answer: \n",
    "The favorable outcome weight for Caucasian offenders is 0.8674773180460935 and for African-American offenders 1.1115635757470286. The non favorable weight for Caucasian offenders is 1.207727271406276 and for African-American offenders 0.8984721282182213. This means that means that more emphasis is put on favorable outcomes for African-American offenders and on non-favorable outcomes for Caucasian offenders. This can be seen in the results: the Favorable Outcome Rate of African-Americans has increased (compared to the results of 5.1) and for Caucasians has decreased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3XdW0qF82JC"
   },
   "source": [
    "#### **5.4 Post-processing: Equalized odds**\n",
    "Use the predictions by the first classifier for this post processing part (see lecture slides for more information about post processing for equalized odds).\n",
    "\n",
    "We have the following parameters (A indicates group membership, Y_{hat} the original prediction, Y_{tilde} the prediction of the derived predictor).\n",
    "\n",
    "* `p_00` = P(Y_{tilde} = 1 | Y_{hat} = 0 & A = 0)\n",
    "* `p_01` = P(Y_{tilde} = 1 | Y_{hat} = 0 & A = 1)\n",
    "* `p_10` = P(Y_{tilde} = 1 | Y_{hat} = 1 & A = 0)\n",
    "* `p_11` = P(Y_{tilde} = 1 | Y_{hat} = 1 & A = 1)\n",
    "\n",
    "\n",
    "Normally, the best parameters `p_00, p_01, p_10, p_11` are found with a linear program that minimizes loss between predictions of a derived predictor and the actual labels. In this assignment we will not ask you to do this. Instead, we would like you to follow the next steps to find parameters, post-process the data and check the performance of this classifier with post-processing:\n",
    "\n",
    "1. Generate 5000 different samples of these 4 parameters randomly;\n",
    "2. Write a function (or more) that applies these 4 parameters to postprocess the predictions.\n",
    "3. For each generated set of 4 parameters:\n",
    "  - Change the predicted labels with the function(s) from step 2;\n",
    "  - Evaluate these 'new' predictions, by calculating group-wise TPR and FPR, as well as overall performance based on F1 and/or accuracy.\n",
    "4. Choose the best set of parameters. Take into account the equalized odds fairness measure, as well a performance measure like accuracy or F1.\n",
    "5. Check the overall performance (precision, recall, accuracy, F1, etc.) of the new predictions after post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Nk_scQdM76He"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0.6394267984578837, (0, 1): 0.025010755222666936, (1, 0): 0.27502931836911926, (1, 1): 0.22321073814882275}\n"
     ]
    }
   ],
   "source": [
    "# Your code for step 1\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# predictions for the first model\n",
    "y_pred = pd.Series(clf1.predict(X_test))\n",
    "\n",
    "# Generate random parameters for the model\n",
    "random_parameters = []\n",
    "for _ in range(5000):\n",
    "  p_00 = random.uniform(0, 1)\n",
    "  p_01 = random.uniform(0, 1)\n",
    "  p_10 = random.uniform(0, 1)\n",
    "  p_11 = random.uniform(0, 1)\n",
    "  random_parameters.append({(0, 0): p_00,\n",
    "                            (0, 1): p_01,\n",
    "                            (1, 0): p_10,\n",
    "                            (1, 1): p_11})\n",
    "\n",
    "# Example, first set of random parameters\n",
    "print(random_parameters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "W0p92txObr6v"
   },
   "outputs": [],
   "source": [
    "# Your code for step 2\n",
    "# Create a dataframe with the necessary information\n",
    "df_post_data = pd.DataFrame({'race_num': X_test['race'],\n",
    "                             'pred_labels': y_pred,\n",
    "                             'true_labels': y_test})\n",
    "\n",
    "# the number of cases falling in each condition\n",
    "subset_sizes = {\n",
    "    (0, 0): len(df_post_data.query('pred_labels == 0 & race_num == 0')),\n",
    "    (0, 1): len(df_post_data.query('pred_labels == 0 & race_num == 1')),\n",
    "    (1, 0): len(df_post_data.query('pred_labels == 1 & race_num == 0')),\n",
    "    (1, 1): len(df_post_data.query('pred_labels == 1 & race_num == 1'))\n",
    "\n",
    "}\n",
    "\n",
    "def generate_labels(subset_sizes, p_dict):\n",
    "    \"\"\"\n",
    "    subset_sizes: dict with number of cases falling in each condition\n",
    "    p_dict: the postprocessing parameters\n",
    "    \"\"\"\n",
    "    new_predictions = {}\n",
    "\n",
    "    for (prediction, group), p in p_dict.items():\n",
    "\n",
    "      # The number of instances for which we need to generate labels\n",
    "      num_instances = subset_sizes[(prediction, group)]\n",
    "      \n",
    "      # Write your code here.\n",
    "      # Get labels and prediction of the current subgroup\n",
    "      y_tilde_for_subgroup = df_post_data.query(f'pred_labels == {prediction} & race_num == {group}')['pred_labels'].values\n",
    "\n",
    "      # flip the labels according to the postprocessing parameters\n",
    "      flip_mask = np.random.rand(num_instances) < p\n",
    "      y_tilde_for_subgroup_new = y_tilde_for_subgroup * (1 - flip_mask) + np.abs(y_tilde_for_subgroup-1) * flip_mask\n",
    "      # save the new predictions\n",
    "      new_predictions[(prediction, group)] = y_tilde_for_subgroup_new\n",
    "\n",
    "    return new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "teZWyupiw2iM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the postprocessing are:\n",
      "   (0, 0)    (0, 1)    (1, 0)    (1, 1)  Accuracy        F1  Precision    Recall  African_American_TPR  African_American_FPR  African_American_FOR  Caucasian_TPR  Caucasian_FPR  Caucasian_FOR\n",
      " 0.639427  0.025011  0.275029  0.223211  0.593750  0.582278   0.564151  0.601610              0.758410              0.650000              0.293461       0.300000       0.138996       0.797203\n",
      " 0.736471  0.676699  0.892180  0.086939  0.432765  0.425695   0.406593  0.446680              0.302752              0.476667              0.614035       0.723529       0.698842       0.291375\n",
      " 0.421922  0.029797  0.218638  0.505355  0.609848  0.564482   0.594655  0.537223              0.672783              0.536667              0.392344       0.276471       0.081081       0.841492\n",
      " 0.026536  0.198838  0.649884  0.544941  0.584280  0.402721   0.621849  0.297787              0.278287              0.130000              0.792663       0.335294       0.196911       0.748252\n",
      " 0.220441  0.589266  0.809430  0.006499  0.494318  0.405345   0.453865  0.366197              0.152905              0.166667              0.840510       0.776471       0.652510       0.298368\n",
      " 0.805819  0.698139  0.340251  0.155479  0.461174  0.543705   0.452000  0.682093              0.675841              0.753333              0.287081       0.694118       0.714286       0.293706\n",
      " 0.957213  0.336595  0.092746  0.096716  0.524621  0.602219   0.496732  0.764588              0.905199              0.920000              0.087719       0.494118       0.420849       0.550117\n",
      " 0.847494  0.603726  0.807128  0.729732  0.413826  0.392542   0.383142  0.402414              0.373089              0.633333              0.502392       0.458824       0.509653       0.510490\n",
      " 0.536228  0.973116  0.378534  0.552041  0.478220  0.549469   0.462810  0.676056              0.633028              0.536667              0.413078       0.758824       0.884170       0.165501\n",
      " 0.829405  0.618520  0.861707  0.577352  0.412879  0.394531   0.383302  0.406439              0.360856              0.580000              0.534290       0.494118       0.583012       0.452214\n"
     ]
    }
   ],
   "source": [
    "# Your code for step 3\n",
    "\n",
    "# Create a dataframe with the necessary information to compare the diffrent parameters\n",
    "df_results = pd.DataFrame(random_parameters)\n",
    "df_results['Accuracy'] = 0.0\n",
    "df_results['F1'] = 0.0\n",
    "df_results['Precision'] = 0.0\n",
    "df_results['Recall'] = 0.0\n",
    "df_results['African_American_TPR'] = 0.0\n",
    "df_results['African_American_FPR'] = 0.0\n",
    "df_results['African_American_FOR'] = 0.0 # Favorable Outcome Rate\n",
    "df_results['Caucasian_TPR'] = 0.0\n",
    "df_results['Caucasian_FPR'] = 0.0\n",
    "df_results['Caucasian_FOR'] = 0.0 # Favorable Outcome Rate\n",
    "\n",
    "for index, p_dict in enumerate(random_parameters):\n",
    "\n",
    "  new_predictions = generate_labels(subset_sizes, p_dict)\n",
    "\n",
    "  # replace the predictions\n",
    "  df_copy = df_post_data.copy()\n",
    "\n",
    "  for (pred, group), p in p_dict.items():\n",
    "\n",
    "    new_preds = new_predictions[(pred,group)]\n",
    "    df_copy.loc[(df_post_data['pred_labels'] == pred) &\n",
    "                (df_post_data['race_num'] == group), 'pred_labels'] = new_preds\n",
    "\n",
    "\n",
    "  # evaluate the new predictions and save the scores\n",
    "  # Write your code here.\n",
    "  new_predictions = df_copy['pred_labels'] #.values\n",
    "  labels = df_copy['true_labels'] #.values\n",
    "  df_new_1 = calculate_performance_metrics(labels, new_predictions)\n",
    "  # accuracy\n",
    "  accuracy = df_new_1[df_new_1.index == \"Accuracy\"]['Value'].values[0]\n",
    "  df_results.loc[index, 'Accuracy'] = accuracy\n",
    "  # F1 score\n",
    "  f1 = df_new_1[df_new_1.index == \"F1 Score\"]['Value'].values[0]\n",
    "  # precision\n",
    "  precision = df_new_1[df_new_1.index == \"Precision\"]['Value'].values[0]\n",
    "  df_results.loc[index, 'Precision'] = precision\n",
    "\n",
    "  # recall\n",
    "  recall = df_new_1[df_new_1.index == \"Recall (TPR)\"]['Value'].values[0]\n",
    "  df_results.loc[index, 'Recall'] = recall\n",
    "\n",
    "  df_new_2 = calculate_statistical_parity(X_test, new_predictions, [\"race\"])\n",
    "  df_results.loc[index, 'Caucasian_FOR'] = df_new_2['Favorable Outcome Rate']['Caucasian']\n",
    "  df_results.loc[index, 'African_American_FOR'] = df_new_2['Favorable Outcome Rate']['African-American']\n",
    "\n",
    "  df_results.loc[index, 'F1'] = f1  \n",
    "  df_new_3 = calculate_group_metrics(X_test, labels, new_predictions, [\"race\"])\n",
    "  df_results.loc[index, 'Caucasian_TPR'] = df_new_3['TPR']['Caucasian']\n",
    "  df_results.loc[index, 'Caucasian_FPR'] = df_new_3['FPR']['Caucasian']\n",
    "  df_results.loc[index, 'African_American_TPR'] = df_new_3['TPR']['African-American']\n",
    "  df_results.loc[index, 'African_American_FPR'] = df_new_3['FPR']['African-American']\n",
    "\n",
    "\n",
    "print(\"The results of the postprocessing are:\")\n",
    "print(df_results.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum accuracy: 0.6505681818181818\n",
      "minimum TPR difference:  8.994423457453493e-05\n",
      "minimum FPR difference:  0.00010296010296007019\n",
      "(0, 0)                  0.001490\n",
      "(0, 1)                  0.208137\n",
      "(1, 0)                  0.217804\n",
      "(1, 1)                  0.060960\n",
      "Accuracy                0.615530\n",
      "F1                      0.553846\n",
      "Precision               0.610169\n",
      "Recall                  0.507042\n",
      "African_American_TPR    0.519878\n",
      "African_American_FPR    0.283333\n",
      "African_American_FOR    0.593301\n",
      "Caucasian_TPR           0.482353\n",
      "Caucasian_FPR           0.293436\n",
      "Caucasian_FOR           0.631702\n",
      "TPR_diff                0.037525\n",
      "FPR_diff                0.010103\n",
      "Ranking                 1.567903\n",
      "Name: 4841, dtype: float64\n",
      "Caucasian Favorable Outcome Rate: 0.6317016317016317\n",
      "African American Favorable Outcome Rate: 0.5933014354066986\n",
      "\n",
      "****************************************************************\n",
      "Performance diffrences:\n",
      "Accuracy difference: 0.047\n",
      "Favorable Outcome Rate difference: -0.242, old: 0.281, new: 0.038\n"
     ]
    }
   ],
   "source": [
    "# Your code for step 4 and 5\n",
    "\n",
    "# export the results to a CSV file\n",
    "df_results.to_csv('postprocessing_results.csv', index=False)\n",
    "\n",
    "df_results['TPR_diff'] = np.abs(df_results['Caucasian_TPR'] - df_results['African_American_TPR'])\n",
    "df_results['FPR_diff'] = np.abs(df_results['Caucasian_FPR'] - df_results['African_American_FPR'])\n",
    "df_results['Ranking'] = df_results['Accuracy']  + (1 - df_results['TPR_diff'] - df_results['FPR_diff'])\n",
    "\n",
    "df_sort = df_results.sort_values(by='Ranking', ascending=False)\n",
    "\n",
    "print(f\"maximum accuracy: {df_sort['Accuracy'].max()}\")\n",
    "print(\"minimum TPR difference: \", df_sort['TPR_diff'].min())\n",
    "print(\"minimum FPR difference: \", df_sort['FPR_diff'].min())\n",
    "\n",
    "\n",
    "# best parameters\n",
    "best_parameters = df_sort.iloc[0]\n",
    "print(best_parameters)\n",
    "print(f\"Caucasian Favorable Outcome Rate: {best_parameters['Caucasian_FOR']}\")\n",
    "print(f\"African American Favorable Outcome Rate: {best_parameters['African_American_FOR']}\")\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"Performance diffrences:\")\n",
    "old_accuracy = df_1_1[df_1_1.index == \"Accuracy\"]['Value'].values[0]\n",
    "accuracy_diff = old_accuracy - best_parameters['Accuracy']\n",
    "print(f\"Accuracy difference: {accuracy_diff:.3f}\")\n",
    "\n",
    "old_African_American_FOR =  df_1_2['Favorable Outcome Rate']['African-American']\n",
    "old_Caucasian_FOR =  df_1_2['Favorable Outcome Rate']['Caucasian']\n",
    "\n",
    "diff_FOR = np.abs(best_parameters['Caucasian_FOR'] - best_parameters['African_American_FOR'])\n",
    "old_diff_FOR = np.abs(old_Caucasian_FOR - old_African_American_FOR)\n",
    "diff_diff_FOR = diff_FOR - old_diff_FOR\n",
    "\n",
    "print(f\"Favorable Outcome Rate difference: {diff_diff_FOR:.3f}, old: {old_diff_FOR:.3f}, new: {diff_FOR:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD0weMlkN4Cq"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Describe how you selected the best set of parameters. Furthermore, how do you interpret the best set of parameters that you found? And what do you think of the results of the new classifier?\n",
    ">***Answer***\n",
    "\n",
    "We found the best set of parameters by combining the best (highest) accuracy and best (lowest) equalized odds. Because we wanted the highest of one and lowest of the other, we decided to subtract the equalized odds (TPR dif + FPR dif) from 1, so that the higher the outcome of the equalized odds and accuracy was, the better. \n",
    "The best set of parameters we found were the parameters that most successfully made the differences between the groups smaller without lowering the accuracy too much. \n",
    "The result was what we expected: the differences in favorable outcome ratio became smaller (from 0.281 to 0.038), but the performance dropped a little (-0.047). This often happens because of post processing and this is called the fairness-accuracy tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwtRtf1Pl5_u"
   },
   "source": [
    "#### **Overall discussion**\n",
    "For all 4 classifiers that you trained, describe:\n",
    "- Does this classifier satisfies statistical parity?\n",
    "- Does the classifier satisfy the equal opportunity criterion?\n",
    "\n",
    "Finally, how do the different classifiers compare against each other?\n",
    "\n",
    ">**Answer**\n",
    "\n",
    "5.1\n",
    "This classifier doesn't satisfy statistical parity, because there is a big difference between the favorable outcome rate of Caucasian and African-American offenders (0.75 VS 0.47 respectively). \n",
    "The classifier doesn't satisfy equalized odds either, because there is a big difference between both the TPR (0.68 A VS 0.39 C) and FPR (0.36 A VS 0.15 C).\n",
    "\n",
    "5.2\n",
    "This classifier doesn't satisfy statistical parity, because there is a big difference between the favorable outcome rate of Caucasian and African-American offenders (0.73 VS 0.47 respectively). \n",
    "The classifier doesn't satisfy equalized odds either, because there is a big difference between both the TPR (0.67 A VS 0.36 C) and FPR (0.36 A VS 0.17 C).\n",
    "\n",
    "5.3\n",
    "This classifier doesn't satisfy statistical parity, because there is a big difference between the favorable outcome rate of Caucasian and African-American offenders (0.56 VS 0.65 respectively). This is the only case where the outcome is more favorable for African-American offenders.\n",
    "The classifier doesn't satisfies equalized odds either, because while there isn't a big difference between in the TPR (0.51 A VS 0.53 C), there is a significant difference in FPR (0.17 A VS 0.37 C).\n",
    "\n",
    "5.4\n",
    "This classifier satisfies statistical parity, because there isn't a big difference between the favorable outcome rate of Caucasian and African-American offenders (0.63 VS 0.61 respectively). \n",
    "The classifier satisfies equalized odds, because there isn't a big difference between both the TPR (0.49 A VS 0.47 C) and FPR (0.29 A VS 0.29 C)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUPZN32kOJ9H"
   },
   "source": [
    "### **6. Intersectional fairness**\n",
    "In the questions above `race` was the only protected attribute. However, multiple protected attributes sometimes interact, leading to different fairness outcomes for different combinations of these protected attributes.\n",
    "\n",
    "Now explore the intersectional fairness for protected attributes `race` and `sex` for the first two classifiers from question 5. Make a combination of the `race` and `sex` column, resulting in four new subgroups (e.g., female Caucasian), and report the maximum difference between the subgroups for statistical parity, TPR and FPR.\n",
    "For example, suppose we have four groups with TPRs 0.1, 0.2, 0.3, 0.8, then the maximum difference is 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-siRd0cUH0sN"
   },
   "source": [
    "Your code to evaluate intersectional fairness for Classifier 1:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "mSXG9sBjT-xX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without protected attributes (race & sex)\n",
      "\n",
      "****************************************************************\n",
      "The statistical parity difference for race & sex is:\n",
      "                         Favorable Outcome Rate\n",
      "African-American Female                0.800000\n",
      "African-American Men                   0.406130\n",
      "Caucasian Female                       0.938776\n",
      "Caucasian Men                          0.697885\n",
      "Maximum difference in favorable outcome rate: 0.533\n",
      "\n",
      "****************************************************************\n",
      "The TPR and FPR of the two protected attribute groups:\n",
      "                              TPR       FPR\n",
      "African-American Female  0.279070  0.145161\n",
      "African-American Men     0.739437  0.420168\n",
      "Caucasian Female         0.142857  0.015873\n",
      "Caucasian Men            0.459259  0.193878\n",
      "Maximum difference in TPR: 0.597\n",
      "Maximum difference in FPR: 0.404\n"
     ]
    }
   ],
   "source": [
    "# Your code for intersectional fairness\n",
    "\n",
    "#filtering the data without race:\n",
    "protected_features = ['race', \"sex\"]\n",
    "\n",
    "\n",
    "print(\"Logistic Regression without protected attributes (race & sex)\")\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The statistical parity difference for race & sex is:\")\n",
    "df_1_st_race_sex = calculate_statistical_parity(X_test, y_pred_1, protected_features)\n",
    "pprint.pprint(df_1_st_race_sex)\n",
    "\n",
    "max_diff_FOR_1 = df_1_st_race_sex['Favorable Outcome Rate'].max() - df_1_st_race_sex['Favorable Outcome Rate'].min()\n",
    "print(f\"Maximum difference in favorable outcome rate: {max_diff_FOR_1:.3f}\")\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The TPR and FPR of the two protected attribute groups:\")\n",
    "df_1_groups_race_sex = calculate_group_metrics(X_test, y_test, y_pred_1, protected_features)\n",
    "pprint.pprint(df_1_groups_race_sex)\n",
    "\n",
    "max_diff_TPR_1 = df_1_groups_race_sex['TPR'].max() - df_1_groups_race_sex['TPR'].min()\n",
    "max_diff_FPR_1 = df_1_groups_race_sex['FPR'].max() - df_1_groups_race_sex['FPR'].min()\n",
    "print(f\"Maximum difference in TPR: {max_diff_TPR_1:.3f}\")\n",
    "print(f\"Maximum difference in FPR: {max_diff_FPR_1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz_hNpkGH1ul"
   },
   "source": [
    "Your code to evaluate intersectional fairness for Classifier 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MwvWChS2H79s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logstic Regression without protected attributes (race & sex)\n",
      "\n",
      "****************************************************************\n",
      "The statistical parity difference for race & sex is:\n",
      "                         Favorable Outcome Rate\n",
      "African-American Female                0.800000\n",
      "African-American Men                   0.938776\n",
      "Caucasian Female                       0.408046\n",
      "Caucasian Men                          0.673716\n",
      "Maximum difference in favorable outcome rate: 0.531\n",
      "\n",
      "****************************************************************\n",
      "The TPR and FPR of the two protected attribute groups:\n",
      "                              TPR       FPR\n",
      "African-American Female  0.279070  0.145161\n",
      "African-American Men     0.142857  0.015873\n",
      "Caucasian Female         0.735915  0.420168\n",
      "Caucasian Men            0.488889  0.214286\n",
      "Maximum difference in TPR: 0.593\n",
      "Maximum difference in FPR: 0.404\n"
     ]
    }
   ],
   "source": [
    "# Your code for intersectional fairness\n",
    "\n",
    "#filtering the data without race:\n",
    "protected_features = ['race', \"sex\"]\n",
    "\n",
    "\n",
    "print(\"Logstic Regression without protected attributes (race & sex)\")\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The statistical parity difference for race & sex is:\")\n",
    "df_2_st_race_sex = calculate_statistical_parity(X_test, y_pred_without, protected_features)\n",
    "pprint.pprint(df_2_st_race_sex)\n",
    "\n",
    "max_diff_FOR_2 = df_2_st_race_sex['Favorable Outcome Rate'].max() - df_2_st_race_sex['Favorable Outcome Rate'].min()\n",
    "print(f\"Maximum difference in favorable outcome rate: {max_diff_FOR_2:.3f}\")\n",
    "\n",
    "print(\"\\n****************************************************************\")\n",
    "print(\"The TPR and FPR of the two protected attribute groups:\")\n",
    "df_2_groups_race_sex = calculate_group_metrics(X_test, y_test, y_pred_without, protected_features)\n",
    "pprint.pprint(df_2_groups_race_sex)\n",
    "\n",
    "max_diff_TPR_2 = df_2_groups_race_sex['TPR'].max() - df_2_groups_race_sex['TPR'].min()\n",
    "max_diff_FPR_2 = df_2_groups_race_sex['FPR'].max() - df_2_groups_race_sex['FPR'].min()\n",
    "print(f\"Maximum difference in TPR: {max_diff_TPR_2:.3f}\")\n",
    "print(f\"Maximum difference in FPR: {max_diff_FPR_2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyE6wo8EH-E0"
   },
   "source": [
    "**Question**\n",
    "\n",
    "Write down a short interpretation of the results you calculated. What do you see?\n",
    "> Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lfFYnU1V_bl"
   },
   "source": [
    "## Discussion\n",
    "Provide a short ethical discussion (1 or 2 paragraphs) reflecting on these two aspects:\n",
    "\n",
    "1) The use of a ML system to try to predict recidivism;\n",
    "\n",
    "2) The public release of a dataset like this.\n",
    "\n",
    "Answer: \n",
    "It can by tricky to use a ML system to try to predict recidivism for a couple of reasons. \n",
    "Firstly, systematic biases (or: unjustices) ofsociety can end up in the data, making the data biased.\n",
    "Secondly, the stakes are high and offenders who get a high risk score, deserve an explanation of why they received that score. This is not possible or at least very complicated when using a ML system.\n",
    "Lastly, using a ML system doesn't necessarily mean better. Scientist Dressel and Farid found that lay people could predict recidivism as accurate as the COMPAS system (https://www-science-org.utrechtuniversity.idm.oclc.org/doi/pdf/10.1126/sciadv.aao5580\n",
    "). However, the COMPAS system is presented as this state-of-the-art ML system, granting it more authority than suitable. \n",
    "While the COMPAS system was ultimately labeled as not being racist, we still have strong doubts about whether or not we should use a system like this.\n",
    "\n",
    "The public release of this dataset has helped researchers get to the bottom of this. However, we don't understand why people's personal details such as their names were included. It would have been better if those where kept anonymously. \n",
    "But, in this case it was vey important that scientist had access to the data, so it was ultimately worth it. It is not often that a dataset like this is released publicly, so the fact that in this case it was, stresses the importance of finding out whether the algorithm was racist or not."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
